#!/usr/bin/env python3
"""
ğŸ–ï¸ BRIGADE - Coordinated Code Intelligence

Where specialized AI agents unite for comprehensive code analysis and enhancement.
Multi-agent coordination for superior code quality.

Usage:
    brigade analyze myfile.py
    brigade auto-fix myfile.py --create-pr  
    brigade deploy myfile.py --mode coordinated
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict

from core.config import Config
from core.exceptions import CodeAnalyzerError
from workflows.workflow_manager import WorkflowManager, WorkflowType


class Brigade:
    """BRIGADE - Coordinated Code Intelligence System"""
    
    def __init__(self):
        self.config = Config.from_env()
        self.workflow_manager = WorkflowManager(self.config)
    
    def create_parser(self) -> argparse.ArgumentParser:
        """Create argument parser"""
        parser = argparse.ArgumentParser(
            prog='brigade',
            description="ğŸ–ï¸ BRIGADE - Coordinated Code Intelligence",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  brigade analyze myfile.py
  brigade auto-fix myfile.py --create-pr
  brigade deploy myfile.py --mode coordinated
  brigade analyze src/ --recursive --output results.json
            """
        )
        
        # Subcommands
        subparsers = parser.add_subparsers(dest='command', help='Available commands')
        
        # Analysis command
        analyze_parser = subparsers.add_parser('analyze', help='Deploy analysis agents')
        self._add_common_args(analyze_parser)
        
        # Auto-fix command
        autofix_parser = subparsers.add_parser('auto-fix', help='Deploy fix agents with PR creation')
        self._add_common_args(autofix_parser)
        autofix_parser.add_argument('--create-pr', action='store_true', help='Create pull request')
        autofix_parser.add_argument('--dry-run', action='store_true', help='Preview fixes only')
        
        # Deploy command (multi-agent coordination)
        deploy_parser = subparsers.add_parser('deploy', help='Deploy coordinated agent brigade')
        self._add_common_args(deploy_parser)
        deploy_parser.add_argument('--mode', choices=['analysis', 'coordinated', 'full'], 
                                  default='coordinated', help='Brigade deployment mode')
        
        # Test command
        test_parser = subparsers.add_parser('test', help='AI-powered testing capabilities')
        test_parser.add_argument('path', help='File or directory to test')
        test_parser.add_argument('--generate', action='store_true',
                               help='Generate AI-powered test cases')
        test_parser.add_argument('--run', action='store_true',
                               help='Run existing tests')
        test_parser.add_argument('--coverage', action='store_true',
                               help='Include coverage analysis')
        test_parser.add_argument('--test-types', nargs='+',
                               choices=['unit', 'integration', 'edge_case'],
                               default=['unit'],
                               help='Types of tests to generate')
        test_parser.add_argument('--output', '-o', help='Save generated tests to file')
        test_parser.add_argument('--validate', action='store_true',
                               help='Validate test coverage')
        test_parser.add_argument('--config', help='Configuration file path')
        test_parser.add_argument('--verbose', '-v', action='store_true',
                               help='Verbose output')
        
        # Repository analysis command
        repo_parser = subparsers.add_parser('repo', help='Analyze entire repository')
        repo_parser.add_argument('path', help='Repository path to analyze')
        repo_parser.add_argument('--max-chunk-size', type=int, default=50000,
                               help='Maximum chunk size in bytes')
        repo_parser.add_argument('--max-files-per-chunk', type=int, default=20,
                               help='Maximum files per chunk')
        repo_parser.add_argument('--output', '-o', help='Save results to JSON file')
        repo_parser.add_argument('--report', help='Generate markdown report')
        repo_parser.add_argument('--parallel-workers', type=int, default=3,
                               help='Number of parallel analysis workers')
        repo_parser.add_argument('--categories', nargs='+', 
                               choices=['core', 'tests', 'config', 'docs', 'build', 'other'],
                               help='Analyze only specific categories')
        repo_parser.add_argument('--config', help='Configuration file path')
        repo_parser.add_argument('--verbose', '-v', action='store_true',
                               help='Verbose output')
        
        # Approval command (manage pending approvals)
        approval_parser = subparsers.add_parser('approve', help='Manage pending PR approvals')
        approval_parser.add_argument('--list', action='store_true', help='List pending approvals')
        approval_parser.add_argument('--approve', help='Approve specific request by ID')
        approval_parser.add_argument('--deny', help='Deny specific request by ID')
        approval_parser.add_argument('--config', help='Configuration file path')
        approval_parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
        
        return parser
    
    def _add_common_args(self, parser: argparse.ArgumentParser):
        """Add common arguments to parser"""
        parser.add_argument('path', help='File or directory to analyze')
        parser.add_argument('--recursive', '-r', action='store_true', 
                           help='Recursively analyze directory')
        parser.add_argument('--output', '-o', help='Save results to JSON file')
        parser.add_argument('--report', help='Generate markdown report')
        parser.add_argument('--config', help='Configuration file path')
        parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    
    def run(self, args: argparse.Namespace) -> int:
        """Run BRIGADE with parsed arguments"""
        
        try:
            # Load custom config if provided
            if args.config:
                self.config = self._load_config(args.config)
                self.workflow_manager = WorkflowManager(self.config)
            
            # Validate path
            if not Path(args.path).exists():
                print(f"âŒ Target not found: {args.path}")
                return 1
            
            # Execute command
            if args.command == 'analyze':
                return self._deploy_analysis_brigade(args)
            elif args.command == 'auto-fix':
                return self._deploy_autofix_brigade(args)
            elif args.command == 'deploy':
                return self._deploy_coordinated_brigade(args)
            elif args.command == 'test':
                return self._run_testing_brigade(args)
            elif args.command == 'repo':
                return self._analyze_repository(args)
            elif args.command == 'approve':
                return self._manage_approvals(args)
            else:
                print("âŒ No command specified. Use --help for usage.")
                return 1
                
        except CodeAnalyzerError as e:
            print(f"âŒ Brigade error: {e}")
            return 1
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Brigade operation interrupted")
            return 1
        except Exception as e:
            if args.verbose:
                import traceback
                traceback.print_exc()
            print(f"âŒ Unexpected error: {e}")
            return 1
    
    def _deploy_analysis_brigade(self, args: argparse.Namespace) -> int:
        """Deploy analysis brigade"""
        
        print("ğŸ–ï¸ BRIGADE Analysis Deployment")
        print("=" * 40)
        
        if Path(args.path).is_file():
            results = self.workflow_manager.execute_workflow(
                args.path, 
                WorkflowType.ANALYSIS_ONLY
            )
            self._print_analysis_results(results)
        else:
            results = self._analyze_directory(args.path, args.recursive)
            self._print_directory_results(results)
        
        # Save results if requested
        if args.output:
            self._save_results(results, args.output)
        
        return 0 if results.get('success', True) else 1
    
    def _deploy_autofix_brigade(self, args: argparse.Namespace) -> int:
        """Deploy auto-fix brigade"""
        
        print("ğŸ–ï¸ BRIGADE Auto-Fix Deployment")
        print("=" * 40)
        
        if not Path(args.path).is_file():
            print("âŒ Auto-fix brigade only supports single files")
            return 1
        
        workflow_args = {
            'create_pr': args.create_pr,
            'dry_run': args.dry_run
        }
        
        results = self.workflow_manager.execute_workflow(
            args.path,
            WorkflowType.AUTO_FIX,
            **workflow_args
        )
        
        self._print_auto_fix_results(results)
        
        if args.output:
            self._save_results(results, args.output)
        
        return 0 if results.get('success', False) else 1
    
    def _deploy_coordinated_brigade(self, args: argparse.Namespace) -> int:
        """Deploy coordinated multi-agent brigade"""
        
        print("ğŸ–ï¸ BRIGADE Coordinated Deployment")
        print("=" * 50)
        
        if not Path(args.path).is_file():
            print("âŒ Coordinated brigade only supports single files")
            return 1
        
        workflow_args = {'mode': args.mode}
        
        results = self.workflow_manager.execute_workflow(
            args.path,
            WorkflowType.STRANDS_COORDINATED,
            **workflow_args
        )
        
        self._print_brigade_results(results)
        
        if args.output:
            self._save_results(results, args.output)
        
        return 0 if results.get('success', False) else 1
    
    def _manage_approvals(self, args: argparse.Namespace) -> int:
        """Manage pending approvals"""
        from core.approval import ApprovalManager
        
        approval_manager = ApprovalManager()
        
        if args.list:
            print("ğŸ–ï¸ BRIGADE Pending Approvals")
            print("=" * 40)
            
            pending = approval_manager.list_pending_approvals()
            
            if not pending:
                print("âœ… No pending approvals")
                return 0
            
            for approval in pending:
                print(f"\nğŸ“‹ ID: {approval['id']}")
                print(f"ğŸ“ File: {approval['file_path']}")
                print(f"ğŸ”§ Fixes: {len(approval.get('fixes', []))}")
                print(f"ğŸ“… Requested: {approval['timestamp']}")
            
            print(f"\nUse: brigade approve --approve <ID> to approve")
            
        elif args.approve:
            if approval_manager.approve_saved_request(args.approve):
                print(f"âœ… Approval {args.approve} approved")
            else:
                print(f"âŒ Could not approve {args.approve}")
                return 1
        else:
            print("âŒ Use --list or --approve <ID>")
            return 1
        
        return 0
    
    def _analyze_repository(self, args):
        """Analyze entire repository with chunking"""
        from analyzers.repo_analyzer import RepositoryAnalyzer
        
        print(f"ğŸ–ï¸ BRIGADE Repository Analysis")
        print(f"ğŸ“ Target: {args.path}")
        print(f"âš™ï¸ Max chunk size: {args.max_chunk_size} bytes")
        print(f"ğŸ“Š Max files per chunk: {args.max_files_per_chunk}")
        
        try:
            analyzer = RepositoryAnalyzer(
                max_chunk_size=args.max_chunk_size,
                max_files_per_chunk=args.max_files_per_chunk
            )
            
            if args.verbose:
                print("ğŸ” Discovering repository structure...")
            
            results = analyzer.analyze_repository(args.path)
            
            # Display summary
            repo_summary = results['repository_summary']
            print(f"\nğŸ“Š Repository Analysis Complete")
            print(f"ğŸ“ Total files: {repo_summary['total_files']}")
            print(f"ğŸ”¤ Languages: {', '.join(repo_summary['languages'])}")
            print(f"â­ Overall quality: {repo_summary['overall_quality']:.1f}/10")
            
            # Show insights
            print(f"\nğŸ’¡ Key Insights:")
            for insight in results['insights']:
                print(f"   {insight}")
            
            # Show recommendations
            print(f"\nğŸ¯ Recommendations:")
            for rec in results['recommendations']:
                print(f"   {rec}")
            
            # Save results if requested
            if args.output:
                with open(args.output, 'w') as f:
                    json.dump(results, f, indent=2)
                print(f"ğŸ’¾ Results saved to {args.output}")
            
            # Generate report if requested
            if args.report:
                self._generate_repo_report(results, args.report)
                print(f"ğŸ“„ Report generated: {args.report}")
            
            return 0
            
        except Exception as e:
            print(f"âŒ Repository analysis failed: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return 1
    
    def _generate_repo_report(self, results, report_path):
        """Generate markdown report for repository analysis"""
        repo_summary = results['repository_summary']
        
        report = f"""# ğŸ–ï¸ BRIGADE Repository Analysis Report

## ğŸ“Š Repository Overview
- **Total Files**: {repo_summary['total_files']}
- **Languages**: {', '.join(repo_summary['languages'])}
- **Overall Quality**: {repo_summary['overall_quality']:.1f}/10

## ğŸ’¡ Key Insights
"""
        
        for insight in results['insights']:
            report += f"- {insight}\n"
        
        report += "\n## ğŸ¯ Recommendations\n"
        for rec in results['recommendations']:
            report += f"- {rec}\n"
        
        report += "\n## ğŸ“‹ Analysis by Category\n"
        for category, summary in results['analysis_by_category'].items():
            report += f"\n### {category.title()}\n"
            report += f"- Files: {summary.get('file_count', 0)}\n"
            report += f"- Average Quality: {summary.get('average_quality', 0):.1f}/10\n"
            
            if summary.get('issue_summary'):
                report += "- Issues:\n"
                for issue_type, count in summary['issue_summary'].items():
                    report += f"  - {issue_type}: {count}\n"
        
        report += f"\n## ğŸ” Issue Summary\n"
        for issue_type, count in results['issue_summary'].items():
            report += f"- **{issue_type}**: {count} issues\n"
        
        report += f"\n---\n*Generated by BRIGADE Repository Analyzer*"
        
        with open(report_path, 'w') as f:
            f.write(report)
    
    def _run_testing_brigade(self, args):
        """Run AI-powered testing capabilities"""
        from analyzers.test_analyzer import TestAnalyzer
        
        print(f"ğŸ§ª BRIGADE Testing Brigade")
        print(f"ğŸ“ Target: {args.path}")
        
        try:
            analyzer = TestAnalyzer()
            
            if args.generate:
                print("ğŸ¤– Generating AI-powered test cases...")
                generated_tests = analyzer.generate_tests(args.path, args.test_types)
                
                print(f"âœ… Generated {len(generated_tests)} test cases")
                
                for test in generated_tests:
                    print(f"\nğŸ“ {test.test_name} ({test.test_type})")
                    print(f"   Confidence: {test.confidence:.1f}")
                    print(f"   Description: {test.description}")
                
                if args.output:
                    self._save_generated_tests(generated_tests, args.output)
                    print(f"ğŸ’¾ Tests saved to {args.output}")
            
            elif args.run:
                print("ğŸƒ Running tests...")
                test_result = analyzer.run_tests(args.path, args.coverage)
                
                print(f"\nğŸ“Š Test Results:")
                print(f"   Status: {'âœ… PASSED' if test_result.passed else 'âŒ FAILED'}")
                print(f"   Tests: {test_result.test_count}")
                
                if test_result.coverage:
                    print(f"   Coverage: {test_result.coverage:.1f}%")
                
                if test_result.failures:
                    print(f"\nâŒ Failures:")
                    for failure in test_result.failures:
                        print(f"   {failure}")
                
                if args.verbose:
                    print(f"\nğŸ“‹ Full Output:")
                    print(test_result.output)
            
            elif args.validate:
                print("ğŸ” Validating test coverage...")
                # Assume test file exists alongside source
                test_path = args.path.replace('.py', '_test.py')
                coverage_info = analyzer.validate_test_coverage(args.path, test_path)
                
                print(f"ğŸ“Š Coverage: {coverage_info['coverage']:.1f}%")
                
                if coverage_info['recommendations']:
                    print(f"\nğŸ’¡ Recommendations:")
                    for rec in coverage_info['recommendations']:
                        print(f"   {rec}")
            
            else:
                # Default: analyze testability
                print("ğŸ” Analyzing code testability...")
                analysis = analyzer.analyze_testability(args.path)
                
                print(f"\nğŸ“Š Testability Analysis:")
                print(f"   Score: {analysis['testability_score']:.1f}/10")
                
                if analysis['testing_issues']:
                    print(f"\nâš ï¸ Testing Issues:")
                    for issue in analysis['testing_issues']:
                        print(f"   {issue['type']}: {issue['description']}")
                
                print(f"\nğŸ’¡ Recommendations:")
                for rec in analysis['test_recommendations']:
                    print(f"   {rec}")
                
                print(f"\nğŸ¯ Suggested Test Types: {', '.join(analysis['suggested_test_types'])}")
            
            return 0
            
        except Exception as e:
            print(f"âŒ Testing failed: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            return 1
    
    def _save_generated_tests(self, tests, output_path):
        """Save generated tests to file"""
        if output_path.endswith('.py'):
            # Save as Python test file
            content = "#!/usr/bin/env python3\n"
            content += '"""AI-generated test cases by BRIGADE"""\n\n'
            content += "import pytest\n\n"
            
            for test in tests:
                content += test.test_code + "\n\n"
            
            with open(output_path, 'w') as f:
                f.write(content)
        else:
            # Save as JSON
            test_data = []
            for test in tests:
                test_data.append({
                    'name': test.test_name,
                    'code': test.test_code,
                    'type': test.test_type,
                    'confidence': test.confidence,
                    'description': test.description
                })
            
            with open(output_path, 'w') as f:
                json.dump(test_data, f, indent=2)
    
    def _analyze_directory(self, directory: str, recursive: bool) -> Dict[str, Any]:
        """Analyze all files in directory"""
        from core.utils import FileUtils
        
        files = FileUtils.find_files(directory, self.config.supported_extensions, recursive)
        
        if not files:
            return {'success': False, 'error': 'No supported files found'}
        
        results = []
        for file_path in files:
            result = self.workflow_manager.execute_workflow(file_path, WorkflowType.ANALYSIS_ONLY)
            results.append(result)
        
        return {
            'success': True,
            'directory': directory,
            'files_analyzed': len(files),
            'results': results
        }
    
    def _print_analysis_results(self, results: Dict[str, Any]):
        """Print analysis results"""
        if not results.get('success'):
            print(f"âŒ Analysis failed: {results.get('error', 'Unknown error')}")
            return
        
        analysis = results['analysis']
        print(f"ğŸ¯ Target: {analysis['file_path']}")
        print(f"ğŸ”¤ Language: {analysis['language'].upper()}")
        print(f"ğŸ“Š Quality Score: {analysis['quality_score']}/10")
        print(f"ğŸš¨ Issues Found: {analysis['issues_found']}")
        
        if analysis['recommendations']:
            print(f"\nğŸ’¡ Brigade Recommendations:")
            for i, rec in enumerate(analysis['recommendations'][:3], 1):
                print(f"   {i}. {rec}")
    
    def _print_auto_fix_results(self, results: Dict[str, Any]):
        """Print auto-fix results"""
        if not results.get('success'):
            print(f"âŒ Auto-fix brigade failed: {results.get('error', 'Unknown error')}")
            return
        
        print(f"âœ… Auto-fix brigade completed successfully")
        if results.get('pr_url'):
            print(f"ğŸ”— Pull Request: {results['pr_url']}")
    
    def _print_brigade_results(self, results: Dict[str, Any]):
        """Print brigade coordination results"""
        if not results.get('success'):
            print(f"âŒ Brigade deployment failed: {results.get('error', 'Unknown error')}")
            return
        
        print(f"âœ… Brigade deployment completed successfully")
        print(f"ğŸ¤– Agents deployed: {results.get('agents_used', 'N/A')}")
    
    def _print_directory_results(self, results: Dict[str, Any]):
        """Print directory analysis results"""
        if not results.get('success'):
            print(f"âŒ Directory analysis failed: {results.get('error', 'Unknown error')}")
            return
        
        print(f"ğŸ“‚ Directory: {results['directory']}")
        print(f"ğŸ“‹ Files analyzed: {results['files_analyzed']}")
        
        # Calculate summary statistics
        total_issues = 0
        total_quality = 0
        
        for result in results['results']:
            if result.get('success'):
                analysis = result['analysis']
                total_issues += analysis['issues_found']
                total_quality += analysis['quality_score']
        
        avg_quality = total_quality / len(results['results']) if results['results'] else 0
        
        print(f"ğŸ“Š Average Quality: {avg_quality:.1f}/10")
        print(f"ğŸš¨ Total Issues: {total_issues}")
    
    def _save_results(self, results: Dict[str, Any], output_path: str):
        """Save results to file"""
        try:
            with open(output_path, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"ğŸ’¾ Results saved to: {output_path}")
        except Exception as e:
            print(f"âŒ Failed to save results: {e}")
    
    def _load_config(self, config_path: str) -> Config:
        """Load configuration from file"""
        try:
            with open(config_path, 'r') as f:
                config_dict = json.load(f)
            return Config.from_dict(config_dict)
        except Exception as e:
            raise CodeAnalyzerError(f"Failed to load config from {config_path}: {e}")

def main():
    """Main entry point"""
    brigade = Brigade()
    parser = brigade.create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    return brigade.run(args)

if __name__ == "__main__":
    sys.exit(main())
